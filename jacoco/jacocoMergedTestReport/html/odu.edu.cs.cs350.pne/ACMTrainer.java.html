<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>ACMTrainer.java</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">Project-CS350</a> &gt; <a href="index.source.html" class="el_package">odu.edu.cs.cs350.pne</a> &gt; <span class="el_source">ACMTrainer.java</span></div><h1>ACMTrainer.java</h1><pre class="source lang-java linenums">package odu.edu.cs.cs350.pne;

import java.io.File;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;


<span class="nc" id="L9">public class ACMTrainer </span>
{
    public static void main(String[] args) 
    {
        //creates train.arff with sample data(Only runs if file has not been created)
<span class="nc" id="L14">        InitialTrainer trainer = new InitialTrainer();</span>
<span class="nc" id="L15">        trainer.run();</span>
        
        //Verify Input from User
<span class="nc bnc" id="L18" title="All 2 branches missed.">        if (!isValidInputs(args))</span>
<span class="nc" id="L19">            System.exit(1);</span>

        //String vocabPath = &quot;src/main/resources/vocabCollection.txt&quot;;
        //String arffPath = &quot;src/main/resources/train.arff&quot;;
<span class="nc" id="L23">        String vocabPath = &quot;acmclass-data/vocabCollection.txt&quot;;</span>
<span class="nc" id="L24">        String arffPath = &quot;acmclass-data/train.arff&quot;;</span>
<span class="nc" id="L25">        StopList stopList = new StopList();</span>

        //DECODE AND PROCESS Documents from ARFF
<span class="nc" id="L28">        ArffParser arffParser = new ArffParser(vocabPath);</span>
<span class="nc" id="L29">        List&lt;Document&gt; documents = arffParser.run(arffPath);</span>

        //CREATE AND PROCESS NEW Inputted Documents
<span class="nc" id="L32">        TrainParser tParser = new TrainParser(stopList, vocabPath);</span>
<span class="nc" id="L33">        TrainDocument inputtedDocument = tParser.createDocument(args[0], args[1].toUpperCase());</span>
        ///tParser.processDocument(inputtedDocument);
        //tParser.writeOutVocabWrapper(vocabPath);

        //CALCULATE TERMWEIGHTS (doing all docs together so we have completed vocab)
<span class="nc" id="L38">        VocabCollection vocab = tParser.getVocabCollection();</span>
<span class="nc" id="L39">        vocab.calcInverseDocFrequency();</span>
<span class="nc" id="L40">        vocab.writeOutVocab(vocabPath);</span>
<span class="nc" id="L41">        documents.add((Document)inputtedDocument);</span>
<span class="nc bnc" id="L42" title="All 2 branches missed.">        for (Document doc: documents)</span>
<span class="nc" id="L43">            doc.calculateTermWeights(vocab);</span>
        
        //ENCODE NEW ARFF from Inputted Documents and ARFF Documents
<span class="nc" id="L46">        ArffGenerator generator = new ArffGenerator(arffPath, vocab);</span>
<span class="nc" id="L47">        generator.generateInstances(documents);</span>
<span class="nc" id="L48">        generator.outputToArff();</span>
        
        //EXIT
<span class="nc" id="L51">    }</span>
    
    //Single File
    private static boolean isValidInputs(String[] args)
    {
<span class="nc bnc" id="L56" title="All 2 branches missed.">        if (args.length != 2)</span>
        {
<span class="nc" id="L58">            System.out.println(&quot;Please include a single file path followed by a specified category.\n&quot;);</span>
<span class="nc" id="L59">            return false;</span>
        }
        
<span class="nc" id="L62">        String path = args[0];</span>
<span class="nc bnc" id="L63" title="All 2 branches missed.">        if (!isValidPath(path))</span>
<span class="nc" id="L64">            return false;   </span>

<span class="nc" id="L66">        String category = args[1];</span>
<span class="nc bnc" id="L67" title="All 2 branches missed.">        if (!isValidCategory(category))</span>
<span class="nc" id="L68">            return false;</span>

<span class="nc" id="L70">        return true;</span>
    }

    private static boolean isValidPath(String path)
    {
<span class="nc" id="L75">        File file = new File(path);</span>
<span class="nc bnc" id="L76" title="All 2 branches missed.">        if (!file.exists())</span>
        {
<span class="nc" id="L78">            System.out.println(path + &quot; does not exist.\n&quot;);</span>
<span class="nc" id="L79">            return false;</span>
        }
<span class="nc bnc" id="L81" title="All 2 branches missed.">        if (file.isDirectory())</span>
        {
<span class="nc" id="L83">            System.out.println(path + &quot; is a directory, not a file.\n&quot;);</span>
<span class="nc" id="L84">            return false;</span>
        }
<span class="nc bnc" id="L86" title="All 2 branches missed.">        if (file.length()==0)</span>
        {
<span class="nc" id="L88">            System.out.println(path + &quot; is empty and cannot be classified.\n&quot;);</span>
<span class="nc" id="L89">            return false;</span>
        }
<span class="nc" id="L91">        String extension = path.substring(path.length()-3);</span>
<span class="nc bnc" id="L92" title="All 4 branches missed.">        if (extension!=&quot;pdf&quot; &amp;&amp; extension!=&quot;txt&quot;)</span>
        {
<span class="nc" id="L94">            System.out.println(path + &quot;: file needs to be a pdf or txt file. \n&quot;);</span>
<span class="nc" id="L95">            return false;</span>
        }
<span class="nc" id="L97">        return true;</span>
    }

    //Verifies a single category
    private static boolean isValidCategory(String category)
    {
<span class="nc" id="L103">        String[] validCategories = {&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;, &quot;F&quot;, &quot;G&quot;, &quot;H&quot;, &quot;I&quot;, &quot;J&quot;, &quot;K&quot;};</span>
<span class="nc" id="L104">        List&lt;String&gt; validCatList = new ArrayList&lt;String&gt;();</span>
<span class="nc" id="L105">        validCatList.addAll(Arrays.asList(validCategories));</span>

<span class="nc bnc" id="L107" title="All 2 branches missed.">        if (validCatList.contains(category.toUpperCase()))</span>
<span class="nc" id="L108">            return true;</span>
        
<span class="nc" id="L110">        System.out.println(&quot;Supplied Category must be within the range A-K according to the ACM Classification System.\n&quot;);</span>
<span class="nc" id="L111">        return false;</span>
    }
    
    
}

//TODOS (In Order of Importance)
    //WAITING FOR SAMPLE FILES (Wendnesday)
        //input sample filepaths and associated categories into InitTrainer
        //InitTrainer.constructInitialDataTest (Tabled till sample files available)
        //Learning Machine Optimization (Tabled till Data Hardcoded or Sample Files provided)
            //Optimize gamma and C values of SMO (Section 5.4 in Design Notes)
            //Test machine accuracy using N-fold Cross Validation (Section 5.3 in Design Notes)
        //LearningMachine.classifyDataTest (Tabled till learning machine optimized)
    //General 
        //Integration Tests
            //Make a folder called integrationTests in src
            //test all main() and run() functions
        //FilePaths
            //move everything except vocabCollection outside of jar
        //Creation of ZIP file
            //use github actions and/or gradle to create zip file dynamically on push
        //Javadoc
            //label all functions using javadoc notation
        //CleanUp
            //eliminate as many Todo's as possible
            //remove commented out code below classes (do this list last)
        //Optional (Tabled)
            //implement multiple categories (Design issue NEEDED but do we wanna take the L)
            //Explore TF-IDF instead of binary-IDF
            //Seperate classes in packages

                
//Possible Document Types: TrainDocument, ClassifyDocument,        ARFFTrainDocument, 
//Possible Parser Types: TrainParser, ClassifyParser,              ARFFParser


//FEATURE FLOW
//Train 
    //(FIRST TIME)read in sample train documents, create, and process (FirstTime)(but dont need store in object)
        //readin, create document with rawtext, splitText OMITTING STOPWORDS into wordList, addToVocab to create initial vocabCollection,
        //createTokenMap with vocabCollection keys, count occurences of each key in the wordList (stored as values), 
        //calculate idf, normalize, term weights, 
        //(TRACK WORDS IN ALL DOCS (keep track of words occuring in all documents as will result in termweight of 0 bc idf == log(N/df) == 0; maybe call from outside parser)), 
        //writeOutVocab
        //create ARFF (with vocabKeys as attributes and the term weights as the attribute values)

    //read in NEW Traindocuments, create, and process
        //readin, readInVocab, create document with rawtext, splitText OMITTING STOPWORDS into wordList, addToVocab to previous vocabCollection,
        //createTokenMap with new vocabCollection keys, count occurences of each key in the wordList (stored as values), 
        //calculate idf, normalize, term weights, 
        //(DONT TRACK WORDS IN ALL DOCS (bc we need previous values for ARFFDoc Construction; maybe call from outside parser)),
        //writeOutVocab

    //create ARFFDocuments with current arff info using NEW vocabMap as keys
        //readinARFF, readInVocab, createTempTokenMap with NEW vocabCollection keys, 
        //for normalized count values, see below
        //calculate idf and termweights, 
        //(NOW TRACK WORDS IN ALL DOCS (to update VocabCollection))
        //(writeOutVocab to update VocabCollection)

    //*******(AT THIS STAGE, we have documents and trainDocuments with n+m keys 
                //(n=previousNumOfVocabWords)(m=wordsThatWereNotInPreviousVocab))

    //Create ARFF with new documents and old train documents using new info and new number of attributes

//NOTES:
//TODO: issue can be handled when reading in arffs if we track vocab words in all documents.
//Already created WordFrequency.isInAllDocuments and VocabCollection.trackWordsInAllDocs()
    //Need to edit read/write functions for VocabCollection include isInAllDocuments member
// if (weight == 0)
// {
//     if (thatVocabWord.isInAllDocuments == true)
//         word.normalizedCount = 1;
//     else
//         word.normalizedCount = 0;
// }
// else
//     word.normalizedCount = 1;

//Loop through values of vocabWords (simulateVocabConstruction())
            /*
             * this function only adds word if vocabWord[v] in doc occurs more than 5 times(normalized count == 1). 
             * However, in trainingDoc construction, each word is added to vocab if its 
             * count is more than 0 (if it occurs at all and not in stopList)
             * 
             * EXAMPLE:
             * weight == norm * idf
             * weight == norm * log_2(N/df)
             * therefore, if word appears in all docs N/df == 1 -&gt; idf == 0
             *    if, all docs have this word less than 5 times -&gt; norm == 0
             *        weight == 0 * 0 (insignificant effect)
             *    if, one document has word more than 5 times -&gt; norm == 1
             *        weight == 1 * 0 (SIGNIFICANT bc we are showing a weight of 0 for a word that is significant ot signature)
             *    if, one document is added with word more than 5 times -&gt; norm == 1
             *        weight == 1 * 0 (SIGNIFICANT bc we are showing a weight of 0 for a word that is significant ot signature)
             * 
             * QUESTION: Do we want to add words to vocab if they appear once?
             *           Or, do we want to add words if they appear more than 5 times (normalized count == 1)?
             */
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.8.7.202105040129</span></div></body></html>